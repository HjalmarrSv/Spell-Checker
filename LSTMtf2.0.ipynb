{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spell Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to build a model that can take a sentence with spelling mistakes as input, and output the same sentence, but with the mistakes corrected. Data can be found on as books from [Project Gutenberg](http://www.gutenberg.org/ebooks/search/?sort_order=downloads) or as cleaned wikipedia dumps.\n",
    "\n",
    "To save time for multiple runs, file are saved in folder \"./data\" and weights of the neural network are saved and reused. Note that reusing only works for the same symbol set and neural net configuration. If you have varying symbols in different data sets (books), run all of them at the same time (put them all in the books folder), and use the offset variable to chew through all of the data piece by piece.\n",
    "\n",
    "This is a one layer LSTM network, that only works forward. Improvements could be to add additional layers, and to use a bidirectional LSTM. More like the human way.\n",
    "\n",
    "The sections of the project are:\n",
    "- Loading the Data\n",
    "- Preparing the Data\n",
    "- Building the Model\n",
    "- Training the Model\n",
    "- Fixing Misspelled sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check hardware. Please note that a GPU is needed if you want to run anything but the smallest datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9287504794949069077\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4967563264\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14461268740794714468\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading of books, encoding needs to be defined \n",
    "# for other os than linux.\n",
    "\n",
    "def load_book(path):\n",
    "    \"\"\"Load a book from its file\"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, encoding='utf-8') as f:\n",
    "        book = f.read()\n",
    "    return book\n",
    "\n",
    "# For loading of precomputed data\n",
    "def load_data(file, path):\n",
    "    \"\"\"Load data from its file\"\"\"\n",
    "    loaded_data = []\n",
    "    input_file = os.path.join(path, file)\n",
    "    print (input_file)\n",
    "    # Load and decompress data from file\n",
    "    with bz2.open(\"./data/sentences.bz2\", \"rt\") as f:\n",
    "        loaded_data = f.read()\n",
    "    return loaded_data\n",
    "\n",
    "def load_vocab_input(file, path):\n",
    "    \"\"\"Load data from its file\"\"\"\n",
    "    input_characters = set()\n",
    "    input_file = os.path.join(path, file)\n",
    "    # Load and decompress data from file\n",
    "    with bz2.open(\"./data/vocab_input.bz2\", \"rt\") as f:\n",
    "        loaded_data = f.read()\n",
    "    for char in loaded_data:\n",
    "        input_characters.add(char)\n",
    "    return input_characters\n",
    "\n",
    "def load_vocab_target(file, path):\n",
    "    \"\"\"Load data from its file\"\"\"\n",
    "    target_characters = set()\n",
    "    input_file = os.path.join(path, file)\n",
    "    # Load and decompress data from file\n",
    "    with bz2.open(\"./data/vocab_target.bz2\", \"rt\") as f:\n",
    "        loaded_data = f.read()\n",
    "    for char in loaded_data:\n",
    "        target_characters.add(char)\n",
    "    return target_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to subfolders\n",
    "path = './books/'\n",
    "path_data = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sentences.bz2\n",
      "No sentences found to load. Going to load books.\n",
      "Loaded vocab_input.\n",
      "Loaded vocab_target.\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "# Check if data files exists\n",
    "# Using try is a cheat. Properly done the code would check if flie exists.\n",
    "try:\n",
    "    loaded_data = load_data('sentences.bz2', path_data)\n",
    "    sentences_exist = True\n",
    "    print('Loaded sentences.')\n",
    "    sentences = []\n",
    "    for line in loaded_data.splitlines():\n",
    "        sentences.append(line)\n",
    "except:\n",
    "    sentences_exist = False\n",
    "    print('No sentences found to load. Going to load books.')\n",
    "    \n",
    "try:\n",
    "    vocab_input = load_vocab_input('vocab_input.bz2', path_data)\n",
    "    vocab_input_exists = True\n",
    "    print('Loaded vocab_input.')\n",
    "    vocab_target = load_vocab_target('vocab_target.bz2', path_data)\n",
    "    print('Loaded vocab_target.')\n",
    "    vocab_target_exists = True\n",
    "except:\n",
    "    vocab_target_exists = vocab_input_exists = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define file loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all of the book file names\n",
    "if sentences_exist == False:\n",
    "    book_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    book_files = book_files[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the books using the file names\n",
    "if sentences_exist == False:\n",
    "    books = []\n",
    "    for book in book_files:\n",
    "        books.append(load_book(path+book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 177535 words in allasou2015nonum1040.sv.\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of words in each book \n",
    "if sentences_exist == False:\n",
    "    for i in range(len(books)):\n",
    "        print(\"There are {} words in {}.\".format(len(books[i].split()), book_files[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppdraget är härmed slutfört.\n",
      "Härigenom föreskrivs följande.\n",
      "Till detta hör ett antal bilagor.\n",
      "Ett begrepp som inte helt är klarlagt.\n",
      "Den enskilde har många roller.\n",
      "Till MB hör ett antal förordningar.\n",
      "Därför är denna lösning inte aktuell.\n",
      "JAMA Facial Plast Surg.\n",
      "Piercing ingår inte i standarden.\n",
      "Flera svar var möjliga att ge.\n",
      "En mottagning lämnade inget svar.\n",
      "Begreppet patient definieras inte.\n",
      "Som en liten del ingår hälsoskyddet.\n",
      "Centrala kapitel för denna utredning är.\n",
      "Resultatet ska dokumenter /.../\n"
     ]
    }
   ],
   "source": [
    "# Check to ensure the text looks alright\n",
    "if sentences_exist == False:\n",
    "    print(books[0][:500],\"/.../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Remove unwanted characters and extra spaces from the text'''\n",
    "    #text = re.sub(r'\\n', ' ', text) \n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
    "    text = re.sub('a0','', text)\n",
    "    text = re.sub('\\'92t','\\'t', text)\n",
    "    text = re.sub('\\'92s','\\'s', text)\n",
    "    text = re.sub('\\'92m','\\'m', text)\n",
    "    text = re.sub('\\'92ll','\\'ll', text)\n",
    "    text = re.sub('\\'91','', text)\n",
    "    text = re.sub('\\'92','', text)\n",
    "    text = re.sub('\\'93','', text)\n",
    "    text = re.sub('\\'94','', text)\n",
    "    #text = re.sub('\\.','. ', text)\n",
    "    #text = re.sub('\\!','! ', text)\n",
    "    #text = re.sub('\\?','? ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    #text = [text.islower()]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text of the books\n",
    "if sentences_exist == False:\n",
    "    clean_books = []\n",
    "    for book in books:\n",
    "        clean_books.append(clean_text(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppdraget är härmed slutfört.\n",
      "Härigenom föreskrivs följande.\n",
      "Till detta hör ett antal bilagor.\n",
      "Ett begrepp som inte helt är klarlagt.\n",
      "Den enskilde har många roller.\n",
      "Till MB hör ett antal förordningar.\n",
      "Därför är denna lösning inte aktuell.\n",
      "JAMA Facial Plast Surg.\n",
      "Piercing ingår inte i standarden.\n",
      "Flera svar var möjliga att ge.\n",
      "En mottagning lämnade inget svar.\n",
      "Begreppet patient definieras inte.\n",
      "Som en liten del ingår hälsoskyddet.\n",
      "Centrala kapitel för denna utredning är.\n",
      "Resultatet ska dokumenter /.../\n"
     ]
    }
   ],
   "source": [
    "# Check to ensure the text has been cleaned properly\n",
    "if sentences_exist == False:\n",
    "    print(clean_books[0][:500],\"/.../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44910 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Split the text from the books into sentences.\n",
    "# Choose whether only lower case or not below.\n",
    "if sentences_exist == False:\n",
    "    sentences = []\n",
    "    save_sentences = \"\"\n",
    "    for book in clean_books:\n",
    "        for sentence in book.splitlines():\n",
    "            sentence = sentence.lower() # lower case to halve the nr of symbols\n",
    "            sentences.append(sentence)\n",
    "            save_sentences += (sentence + '\\n')\n",
    "    print(\"There are {} sentences.\".format(len(sentences)))\n",
    "    # Write compressed data to file\n",
    "    with bz2.open(\"./data/sentences.bz2\", \"wt\") as bzip_file:\n",
    "        unused = bzip_file.write(save_sentences)\n",
    "        #unused = bzip_file.write(save_sentences.encode())     \n",
    "        # encoding=’utf8′, errors=’strict’\n",
    "    save_sentences = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uppdraget är härmed slutfört.', 'härigenom föreskrivs följande.', 'till detta hör ett antal bilagor.', 'ett begrepp som inte helt är klarlagt.', 'den enskilde har många roller.']\n"
     ]
    }
   ],
   "source": [
    "# Check to ensure the text has been split correctly.\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of each sentence\n",
    "lengths = []\n",
    "for sentence in sentences:\n",
    "    lengths.append(len(sentence))\n",
    "lengths = pd.DataFrame(lengths, columns=[\"counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44910.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.283812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.472319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             counts\n",
       "count  44910.000000\n",
       "mean      28.283812\n",
       "std        8.472319\n",
       "min       10.000000\n",
       "25%       21.000000\n",
       "50%       30.000000\n",
       "75%       36.000000\n",
       "max       40.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 44910 sentences to train and test our model.\n"
     ]
    }
   ],
   "source": [
    "# Limit the data we will use to train our model\n",
    "max_length = 60 # was 92\n",
    "min_length = 10\n",
    "\n",
    "good_sentences = []\n",
    "\n",
    "#for sentence in int_sentences:\n",
    "for sentence in sentences:\n",
    "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
    "        good_sentences.append(sentence)\n",
    "\n",
    "print(\"We will use {} sentences to train and test our model.\".format(len(good_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 40419\n",
      "Number of testing sentences: 4491\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sentences (testing is for\n",
    "# inferencing). There will be three data sets; training which is split\n",
    "# into training and validation during fit, and testing for inferencing.\n",
    "# Note that inferencing does not use the test set, as of yet.\n",
    "training, testing = train_test_split(good_sentences, test_size = 0.1, random_state = 2)\n",
    "\n",
    "print(\"Number of training sentences:\", len(training))\n",
    "print(\"Number of testing sentences:\", len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o',\n",
    "           'p','q','r','s','t','u','v','w','x','y','z','å','ä','ö',' ','.']\n",
    "\n",
    "# Note that caps and numbers are not introduced as errors, because the data does not contain them.\n",
    "#           'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P',\n",
    "#           'Q','R','S','T','U','V','X','Y','Z','Å','Ä','Ö',\n",
    "\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    noisy_sentence = \"\"\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0,1,1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence+=sentence[i]\n",
    "        else:\n",
    "            new_random = np.random.uniform(0,1,1)\n",
    "            # ~20% chance characters will swap locations\n",
    "            if new_random > 0.8:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence+=sentence[i+1]\n",
    "                    noisy_sentence+=sentence[i]\n",
    "                    #noisy_sentence.append(sentence[i+1])\n",
    "                    #noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~20% chance an extra letter will be added to the sentence\n",
    "            elif new_random > 0.60:\n",
    "                noisy_sentence+=sentence[i]\n",
    "                noisy_sentence+=sentence[i]\n",
    "                #noisy_sentence.append(sentence[i])\n",
    "                #noisy_sentence.append(sentence[i])\n",
    "                #random_letter = np.random.choice(letters, 1)[0]\n",
    "                #noisy_sentence.append(vocab_to_int[random_letter])\n",
    "            # ~40% chance a letter will be substituted for another\n",
    "            elif new_random > 0.20:\n",
    "                noisy_sentence+=sentence[i]\n",
    "                #noisy_sentence.append(sentence[i])\n",
    "                #random_letter = np.random.choice(letters, 1)[0]\n",
    "                #noisy_sentence.append(vocab_to_int[random_letter])\n",
    "            # 20% chance a character will not be typed\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The noise_maker function is used to create spelling mistakes that are similar to those we would make. Sometimes we forget to type a letter, type a letter in the wrong location, or add an extra letter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uppdraget är härmed slutfört.\n",
      "uppdraget är härmed slutfört.\n",
      "\n",
      "härigenom föreskrivs följande.\n",
      "härigenom föreskrivs följande.\n",
      "\n",
      "till detta hör ett antal bilagor.\n",
      "till detta hör tet antal bilagor.\n",
      "\n",
      "ett begrepp som inte helt är klarlagt.\n",
      "ett begrepp som inte helt är klalragt.\n",
      "\n",
      "den enskilde har många roller.\n",
      "den enskilde  har många orlle.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check to ensure noise_maker is making mistakes correctly.\n",
    "threshold = 0.95\n",
    "# for sentence in training_sorted[:5]:\n",
    "training = sentences\n",
    "for sentence in training[:5]:\n",
    "    print(sentence)\n",
    "    print(noise_maker(sentence, threshold))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# the Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # Limited by memory. 64 is much faster, but uses more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time steps, ie number of letters\n",
    "input_dim = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30 # 100 # Number of epochs to train for. Smaller data sets\n",
    "                  # needs more runs than larger data sets. 1 is only\n",
    "                  # useful for testing. Less is better, an overtrained\n",
    "                  # model is useless.\n",
    "\n",
    "units = 64\n",
    "\n",
    "output_size = 10\n",
    "\n",
    "latent_dim = 256      # Latent dimensionality of the encoding space.\n",
    "num_samples = 100000  # Number of samples to train on. Used with\n",
    "                      # the offset variable you can chew through\n",
    "                      # large data sets, without using much memory.\n",
    "threshold = 0.95      # introduce 5 % errors, i.e. about once for every 20 chars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "if vocab_target_exists:\n",
    "    target_characters = vocab_target\n",
    "    target_characters = sorted(list(target_characters))\n",
    "else: \n",
    "    target_characters = set()\n",
    "\n",
    "if vocab_input_exists:\n",
    "    input_characters = vocab_input\n",
    "    input_characters = sorted(list(input_characters))\n",
    "else:\n",
    "    input_characters = set()\n",
    "\n",
    "lines = training\n",
    "\n",
    "training = []\n",
    "#testing = []\n",
    "good_sentences = []\n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_characters = False\n",
    "# offset for large datasets, if you want to run it piece by piece\n",
    "line_offset = 0\n",
    "\n",
    "for line in lines[line_offset: min((num_samples+line_offset), len(lines) - 1)]:\n",
    "    target_text = str (line)\n",
    "    input_text = noise_maker(line, threshold)\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "            added_characters = True\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            added_characters = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not vocab_input_exists:\n",
    "    print('saving vocab')\n",
    "    input_characters = sorted(list(input_characters))\n",
    "    vocab_input=\"\"\n",
    "    for char in input_characters:\n",
    "        vocab_input += char\n",
    "    with bz2.open(\"./data/vocab_input.bz2\", \"wt\") as bzip_file:\n",
    "        unused = bzip_file.write(vocab_input)\n",
    "    added_characters = False\n",
    "\n",
    "if not vocab_target_exists:\n",
    "    print('saving vocab')\n",
    "    target_characters = sorted(list(target_characters))\n",
    "    vocab_target=\"\"\n",
    "    for char in target_characters:\n",
    "        vocab_target += char\n",
    "    with bz2.open(\"./data/vocab_target.bz2\", \"wt\") as bzip_file:\n",
    "        unused = bzip_file.write(vocab_target)\n",
    "    added_characters = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that added characters (new characters in new datasets) are\n",
    "# save correctly, but read incorrectly and then sorted. This is a bug.\n",
    "# It is better to use a fixed vocabulary (list of symbols), that are \n",
    "# always guaranteed to remain in the same order, even when data \n",
    "# sets (texts/books), contain different characters/symbols.\n",
    "if added_characters:\n",
    "    vocab_input=\"\"\n",
    "    for char in input_characters:\n",
    "        vocab_input += char\n",
    "    with bz2.open(\"./data/vocab_input.bz2\", \"wt\") as bzip_file:\n",
    "        unused = bzip_file.write(vocab_input)\n",
    "    vocab_target=\"\"\n",
    "    for char in target_characters:\n",
    "        vocab_target += char\n",
    "    with bz2.open(\"./data/vocab_target.bz2\", \"wt\") as bzip_file:\n",
    "        unused = bzip_file.write(target_characters)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 44910\n",
      "Number of unique input tokens: 33\n",
      "Number of unique output tokens: 35\n",
      "Max sequence length for inputs: 71\n",
      "Max sequence length for outputs: 73\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max_length + 11 # max([len(txt) for txt in clean_books])\n",
    "max_decoder_seq_length = max_encoder_seq_length + 2 # max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(lines))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '.': 2, '?': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29, 'ä': 30, 'å': 31, 'ö': 32}\n",
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '.': 4, '?': 5, 'a': 6, 'b': 7, 'c': 8, 'd': 9, 'e': 10, 'f': 11, 'g': 12, 'h': 13, 'i': 14, 'j': 15, 'k': 16, 'l': 17, 'm': 18, 'n': 19, 'o': 20, 'p': 21, 'q': 22, 'r': 23, 's': 24, 't': 25, 'u': 26, 'v': 27, 'w': 28, 'x': 29, 'y': 30, 'z': 31, 'ä': 32, 'å': 33, 'ö': 34}\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "print(input_token_index)\n",
    "print(target_token_index)\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for weights",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c0fc9b059257>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Do not run this line if no weights are saved!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name)\u001b[0m\n\u001b[0;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[0;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name)\u001b[0m\n\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1144\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train.NewCheckpointReader'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mthis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_CheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for weights"
     ]
    }
   ],
   "source": [
    "# Do not run this line if no weights are saved!\n",
    "status = model.load_weights('weights') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35927 samples, validate on 8982 samples\n",
      "Epoch 1/30\n",
      "35927/35927 [==============================] - 29s 817us/sample - loss: 0.9736 - val_loss: 0.7931\n",
      "Epoch 2/30\n",
      "35927/35927 [==============================] - 22s 610us/sample - loss: 0.6456 - val_loss: 0.6619\n",
      "Epoch 3/30\n",
      "35927/35927 [==============================] - 22s 605us/sample - loss: 0.5183 - val_loss: 0.5955\n",
      "Epoch 4/30\n",
      "35927/35927 [==============================] - 21s 597us/sample - loss: 0.4493 - val_loss: 0.5402\n",
      "Epoch 5/30\n",
      "35927/35927 [==============================] - 22s 618us/sample - loss: 0.4061 - val_loss: 0.5154\n",
      "Epoch 6/30\n",
      "35927/35927 [==============================] - 22s 602us/sample - loss: 0.3766 - val_loss: 0.4857\n",
      "Epoch 7/30\n",
      "35927/35927 [==============================] - 22s 609us/sample - loss: 0.3523 - val_loss: 0.4665\n",
      "Epoch 8/30\n",
      "35927/35927 [==============================] - 22s 618us/sample - loss: 0.3331 - val_loss: 0.4551\n",
      "Epoch 9/30\n",
      "35927/35927 [==============================] - 22s 626us/sample - loss: 0.3163 - val_loss: 0.4475\n",
      "Epoch 10/30\n",
      "35927/35927 [==============================] - 22s 608us/sample - loss: 0.3019 - val_loss: 0.4361\n",
      "Epoch 11/30\n",
      "35927/35927 [==============================] - 22s 610us/sample - loss: 0.2898 - val_loss: 0.4302\n",
      "Epoch 12/30\n",
      "35927/35927 [==============================] - 23s 628us/sample - loss: 0.2794 - val_loss: 0.4242\n",
      "Epoch 13/30\n",
      "35927/35927 [==============================] - 22s 612us/sample - loss: 0.2700 - val_loss: 0.4246\n",
      "Epoch 14/30\n",
      "35927/35927 [==============================] - 22s 604us/sample - loss: 0.2617 - val_loss: 0.4191\n",
      "Epoch 15/30\n",
      "35927/35927 [==============================] - 22s 611us/sample - loss: 0.2541 - val_loss: 0.4174\n",
      "Epoch 16/30\n",
      "35927/35927 [==============================] - 23s 628us/sample - loss: 0.2472 - val_loss: 0.4158\n",
      "Epoch 17/30\n",
      "35927/35927 [==============================] - 22s 625us/sample - loss: 0.2410 - val_loss: 0.4147\n",
      "Epoch 18/30\n",
      "35927/35927 [==============================] - 22s 612us/sample - loss: 0.2350 - val_loss: 0.4097\n",
      "Epoch 19/30\n",
      "35927/35927 [==============================] - 24s 667us/sample - loss: 0.2294 - val_loss: 0.4189\n",
      "Epoch 20/30\n",
      "35927/35927 [==============================] - 22s 608us/sample - loss: 0.2242 - val_loss: 0.4122\n",
      "Epoch 21/30\n",
      "35927/35927 [==============================] - 22s 604us/sample - loss: 0.2192 - val_loss: 0.4056\n",
      "Epoch 22/30\n",
      "35927/35927 [==============================] - 22s 608us/sample - loss: 0.2144 - val_loss: 0.4094\n",
      "Epoch 23/30\n",
      "35927/35927 [==============================] - 23s 627us/sample - loss: 0.2101 - val_loss: 0.4133\n",
      "Epoch 24/30\n",
      "35927/35927 [==============================] - 22s 623us/sample - loss: 0.2057 - val_loss: 0.4073\n",
      "Epoch 25/30\n",
      "35927/35927 [==============================] - 23s 631us/sample - loss: 0.2017 - val_loss: 0.4056\n",
      "Epoch 26/30\n",
      "35927/35927 [==============================] - 24s 654us/sample - loss: 0.1976 - val_loss: 0.4100\n",
      "Epoch 27/30\n",
      "35927/35927 [==============================] - 23s 633us/sample - loss: 0.1938 - val_loss: 0.4116\n",
      "Epoch 28/30\n",
      "35927/35927 [==============================] - 23s 639us/sample - loss: 0.1905 - val_loss: 0.4079\n",
      "Epoch 29/30\n",
      "35927/35927 [==============================] - 23s 650us/sample - loss: 0.1871 - val_loss: 0.4105\n",
      "Epoch 30/30\n",
      "35927/35927 [==============================] - 24s 659us/sample - loss: 0.1840 - val_loss: 0.4137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22ae144a048>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "model.save_weights('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fixing Custom Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, LSTM, Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: uppdraget är härmed slufört.\n",
      "Decoded sentence: uppdraget är märst avgrågat.\n",
      "\n",
      "-\n",
      "Input sentence: härigenom föreskrivs följande.\n",
      "Decoded sentence: härigenom föreskrivs följande.\n",
      "\n",
      "-\n",
      "Input sentence: till detta hör ett antal  bilagor.\n",
      "Decoded sentence: till det behöver de utföras av slu.\n",
      "\n",
      "-\n",
      "Input sentence: ett begrepp som inte helt är klarlagt.\n",
      "Decoded sentence: ett klimatpolitiskt ramverk för sverige.\n",
      "\n",
      "-\n",
      "Input sentence: dne enskilde har många roller.\n",
      "Decoded sentence: den modell kommer att vara någ.\n",
      "\n",
      "-\n",
      "Input sentence: till mb hör ett antal förordnignar.\n",
      "Decoded sentence: till det kommer att förslaget vält.\n",
      "\n",
      "-\n",
      "Input sentence: därför är denna lösning inte aktuell.\n",
      "Decoded sentence: därför är en styrelse är stor uppgift.\n",
      "\n",
      "-\n",
      "Input sentence: jamafacial plast surg\n",
      "Decoded sentence: jaa stare inte servide.\n",
      "\n",
      "-\n",
      "Input sentence: piercing ingår inte i standarden.\n",
      "Decoded sentence: priviktion and research service.\n",
      "\n",
      "-\n",
      "Input sentence: flera svar var möjliga att ge.\n",
      "Decoded sentence: flera svar möjliga utgångsprax.\n",
      "\n",
      "-\n",
      "Input sentence: en mottagning lämnnade inget svar.\n",
      "Decoded sentence: en som elever på förslag kan tas ås.\n",
      "\n",
      "-\n",
      "Input sentence: begreppet patient definieras iinte.\n",
      "Decoded sentence: begravningsclearing anses andrade.\n",
      "\n",
      "-\n",
      "Input sentence: som en lite ndel ingår hälsoskyddet.\n",
      "Decoded sentence: som en gemensam kommer dock följas.\n",
      "\n",
      "-\n",
      "Input sentence: centrala kapitle för denna utredning är.\n",
      "Decoded sentence: centrala att före på vård av flera skola.\n",
      "\n",
      "-\n",
      "Input sentence: resutlatet  ska dokmenteras.\n",
      "Decoded sentence: resultaten ska vid att domstol.\n",
      "\n",
      "-\n",
      "Input sentence: lagen består av följande kapitel.\n",
      "Decoded sentence: lagen gäller i styrelsen till det.\n",
      "\n",
      "-\n",
      "Input sentence: ehandlingar med strålning m.m.\n",
      "Decoded sentence: nedan till problem annen sker.\n",
      "\n",
      "-\n",
      "Input sentence: huden består av tre lager.\n",
      "Decoded sentence: huvudregelnutginvest ur.\n",
      "\n",
      "-\n",
      "Input sentence: ppersonen kan varafysisk eller juridisk.\n",
      "Decoded sentence: previs of manus inflytand exare newsede.\n",
      "\n",
      "-\n",
      "Input sentence: vreksamheten ska hålla god kalitet.\n",
      "Decoded sentence: verksamheten ska vara okomplick inte.\n",
      "\n",
      "-\n",
      "Input sentence: vekrsamheten ska dessutom vara säker.\n",
      "Decoded sentence: verksamheten införs en språklig skall.\n",
      "\n",
      "-\n",
      "Input sentence: yrrkeesutbildningen är grunden.\n",
      "Decoded sentence: härigens and som en egen bör inte.\n",
      "\n",
      "-\n",
      "Input sentence: patienten ska visas omtank eoch respekt.\n",
      "Decoded sentence: patienten ska visas omtanke och respekt.\n",
      "\n",
      "-\n",
      "Input sentence: människor är olika.\n",
      "Decoded sentence: människon är polis.\n",
      "\n",
      "-\n",
      "Input sentence: kravet ligger indirekt på konsumenten.\n",
      "Decoded sentence: kravet på samhällets anomanisationen.\n",
      "\n",
      "-\n",
      "Input sentence: pul däremot är tillämplig.\n",
      "Decoded sentence: på väg av markorlegsälsa.\n",
      "\n",
      "-\n",
      "Input sentence: texten har anpasstas.\n",
      "Decoded sentence: texten i staten infly.\n",
      "\n",
      "-\n",
      "Input sentence: information eller kommunikation?\n",
      "Decoded sentence: informationen ska vara mångdet s?\n",
      "\n",
      "-\n",
      "Input sentence: tillsnyen timdebiteras.\n",
      "Decoded sentence: tillsyn med skulle anges.\n",
      "\n",
      "-\n",
      "Input sentence: problemet är mångfasettrat.\n",
      "Decoded sentence: problemet är därmed ut i det.\n",
      "\n",
      "-\n",
      "Input sentence: det antas att mörkertalet är omfattnde.\n",
      "Decoded sentence: det andra fall ska dock närmare en del.\n",
      "\n",
      "-\n",
      "Input sentence: utågngspunkten är att nåot måste göras.\n",
      "Decoded sentence: utredningen om angerna är fortfarande.\n",
      "\n",
      "-\n",
      "Input sentence: förfarandet sak dokumenteas.\n",
      "Decoded sentence: förfarandet har inte genomfört.\n",
      "\n",
      "-\n",
      "Input sentence: även externa konsulter anlitades.\n",
      "Decoded sentence: även det syns nedan vid en sådan lös.\n",
      "\n",
      "-\n",
      "Input sentence: utrdaren ska bl.a.\n",
      "Decoded sentence: utredaren ska bl.a.\n",
      "\n",
      "-\n",
      "Input sentence: detta måst eavgöras från fall till fall.\n",
      "Decoded sentence: detta gäller i förare antagniska sina.\n",
      "\n",
      "-\n",
      "Input sentence: ingen av dessa behandlingar.\n",
      "Decoded sentence: ingen andra betydelse i sverige.\n",
      "\n",
      "-\n",
      "Input sentence: ingen av dessa behadnlingar\n",
      "Decoded sentence: ingen andra betydelse i sverige.\n",
      "\n",
      "-\n",
      "Input sentence: ingen av dessa behandlingar.\n",
      "Decoded sentence: ingen andra betydelse i sverige.\n",
      "\n",
      "-\n",
      "Input sentence: ingen av dessa behandlingar.\n",
      "Decoded sentence: ingen andra betydelse i sverige.\n",
      "\n",
      "-\n",
      "Input sentence: mifiid ii och mifir.\n",
      "Decoded sentence: mifid ii och mifir.\n",
      "\n",
      "-\n",
      "Input sentence: rektorn och sstyrkedjan.\n",
      "Decoded sentence: rektorn och styrkedjan.\n",
      "\n",
      "-\n",
      "Input sentence: skapa tilltro.\n",
      "Decoded sentence: skapa tilltro.\n",
      "\n",
      "-\n",
      "Input sentence: utmaningar för ett jämställt arbetsliv.\n",
      "Decoded sentence: utmaningar för ett jämställt arbetsliv.\n",
      "\n",
      "-\n",
      "Input sentence: om mått på livskvaliitet.\n",
      "Decoded sentence: om mått på livskvalitet.\n",
      "\n",
      "-\n",
      "Input sentence: en förvaltnin gsom håler ihop.\n",
      "Decoded sentence: en förvaltning som håller ihop.\n",
      "\n",
      "-\n",
      "Input sentence: förslag till ny lv.\n",
      "Decoded sentence: förslag till ny lvu.\n",
      "\n",
      "-\n",
      "Input sentence: fler och starkare ptienter.\n",
      "Decoded sentence: fler och starkare patienter.\n",
      "\n",
      "-\n",
      "Input sentence: målo ch myndighet.\n",
      "Decoded sentence: mål och myndighet.\n",
      "\n",
      "-\n",
      "Input sentence: en övrsyn av det nuvarrande ysstemt.\n",
      "Decoded sentence: en översyn av det nuvarande systemet.\n",
      "\n",
      "-\n",
      "Input sentence: utbildning förframtidens arbtesamrknad.\n",
      "Decoded sentence: utbildning för framtidens arbetsmarknad.\n",
      "\n",
      "-\n",
      "Input sentence: bilaga till betänkande.\n",
      "Decoded sentence: bilaga till betänkande.\n",
      "\n",
      "-\n",
      "Input sentence: koncentrera vårde för patientens bästa.\n",
      "Decoded sentence: koncernresultat är fortfarande sig all.\n",
      "\n",
      "-\n",
      "Input sentence: miljöbalkens hushållningsbetämmelser.\n",
      "Decoded sentence: miljöbalkens hushållningsbetämmelser.\n",
      "\n",
      "-\n",
      "Input sentence: umaninagr för ett jämställt arbetsliv.\n",
      "Decoded sentence: utmaningar för ett jämställt arbetsliv.\n",
      "\n",
      "-\n",
      "Input sentence: översyn av lex laval.\n",
      "Decoded sentence: översyn av lex laval.\n",
      "\n",
      "-\n",
      "Input sentence: mifid ii och mmifir.\n",
      "Decoded sentence: mifid ii och mifir.\n",
      "\n",
      "-\n",
      "Input sentence: krav på privata aktörer i välfärden.\n",
      "Decoded sentence: krav på privata aktörer i välfärden.\n",
      "\n",
      "-\n",
      "Input sentence: en ny ordning för redovisningstiillsyn\n",
      "Decoded sentence: en ny ordning för redovisningstillsyn.\n",
      "\n",
      "-\n",
      "Input sentence: en kommmunaallag för framtiden.\n",
      "Decoded sentence: en kommunallag för framtiden.\n",
      "\n",
      "-\n",
      "Input sentence: kemikaliekatt.\n",
      "Decoded sentence: kemikalieskatt.\n",
      "\n",
      "-\n",
      "Input sentence: får vi det bättre?\n",
      "Decoded sentence: får vi det bättre?\n",
      "\n",
      "-\n",
      "Input sentence: om mått på livskvalitet.\n",
      "Decoded sentence: om mått på livskvalitet.\n",
      "\n",
      "-\n",
      "Input sentence: ucits v. en uppdaterad fondlagstiftning.\n",
      "Decoded sentence: ucits v. en uppdaterad fondlagstiftning.\n",
      "\n",
      "-\n",
      "Input sentence: energiskatt på el.\n",
      "Decoded sentence: energiskatt på el.\n",
      "\n",
      "-\n",
      "Input sentence: en översyn aav det nuvarande systemet..\n",
      "Decoded sentence: en översyn av det nuvarande systemet.\n",
      "\n",
      "-\n",
      "Input sentence: utbildning för framtidens arbetsmarknad.\n",
      "Decoded sentence: utbildning för framtidens arbetsmarknad.\n",
      "\n",
      "-\n",
      "Input sentence: med fokus på kärnuppgifterna.\n",
      "Decoded sentence: med fokus på kärnuppgifterna.\n",
      "\n",
      "-\n",
      "Input sentence: en översyn av årsredovisningslagarna.\n",
      "Decoded sentence: en översyn av årsredovisningslagarna.\n",
      "\n",
      "-\n",
      "Input sentence: enny äkerhetsskyddslag.\n",
      "Decoded sentence: en ny säkerhetsskyddslag.\n",
      "\n",
      "-\n",
      "Input sentence: översyn av lagen om skiljeförfarande.\n",
      "Decoded sentence: översyn av lagen om skiljeförfarande.\n",
      "\n",
      "-\n",
      "Input sentence: ny patentlag.\n",
      "Decoded sentence: ny patentlag.\n",
      "\n",
      "-\n",
      "Input sentence: nya regler för reviorer och revision.\n",
      "Decoded sentence: nya regler för revisorer och revision.\n",
      "\n",
      "-\n",
      "Input sentence: tillsyn över polisen och kriminalvården.\n",
      "Decoded sentence: tillsyn över polisen och kriminalvården.\n",
      "\n",
      "-\n",
      "Input sentence: för att brott inte ska löna sig.\n",
      "Decoded sentence: för att brott inte ska löna sig.\n",
      "\n",
      "-\n",
      "Input sentence: fakturabedrägerier.\n",
      "Decoded sentence: fakturabedrägerier.\n",
      "\n",
      "-\n",
      "Input sentence: begraavningsclearing.\n",
      "Decoded sentence: begravningsclearing.\n",
      "\n",
      "-\n",
      "Input sentence: medieborgarna medieran.\n",
      "Decoded sentence: medieborgarna medierna.\n",
      "\n",
      "-\n",
      "Input sentence: iblaga tilll betänkande.\n",
      "Decoded sentence: ibland finns inte beröras.\n",
      "\n",
      "-\n",
      "Input sentence: vägar till et effektivare miljöarbete.\n",
      "Decoded sentence: vägar till ett effektivare miljöarbete.\n",
      "\n",
      "-\n",
      "Input sentence: miljöbalkens hushållningsbetämmelser.\n",
      "Decoded sentence: miljöbalkens hushållningsbetämmelser.\n",
      "\n",
      "-\n",
      "Input sentence: uppgiftslämnarserviec för företagen.\n",
      "Decoded sentence: uppgiftslämnarservice för företagen.\n",
      "\n",
      "-\n",
      "Input sentence: koll på nlägningen\n",
      "Decoded sentence: koll på anläggningen.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: eu och kommunernas bosatdspolitik.\n",
      "Decoded sentence: eu och kommunernas bostadspolitik.\n",
      "\n",
      "-\n",
      "Input sentence: delrapport rån sverigeförhandlingen.\n",
      "Decoded sentence: delrapport från sverigeförhandlingen.\n",
      "\n",
      "-\n",
      "Input sentence: en förvaltning som håller ihop.\n",
      "Decoded sentence: en förvaltning som håller ihop.\n",
      "\n",
      "-\n",
      "Input sentence: bostder att ob kvarr i.\n",
      "Decoded sentence: bostadsbrist råder i del.\n",
      "\n",
      "-\n",
      "Input sentence: mer gemensamma tobaksreglerr.\n",
      "Decoded sentence: mer gemensamma tobaksregler.\n",
      "\n",
      "-\n",
      "Input sentence: mer trygghet och bättref örsäkring.\n",
      "Decoded sentence: mer trygghet och bättre försäkring.\n",
      "\n",
      "-\n",
      "Input sentence: systematiska jämförelser.\n",
      "Decoded sentence: systematiska jämförelser.\n",
      "\n",
      "-\n",
      "Input sentence: för  lärande i staten.\n",
      "Decoded sentence: för lärande i staten.\n",
      "\n",
      "-\n",
      "Input sentence: arbetslöhet och ekonomisk iståånd..\n",
      "Decoded sentence: arbetslöhet och ekonomiskt bistånd.\n",
      "\n",
      "-\n",
      "Input sentence: skapa tilltro.\n",
      "Decoded sentence: skapa tilltro.\n",
      "\n",
      "-\n",
      "Input sentence: bans och ungasr ätt vid tvångsvård.\n",
      "Decoded sentence: barns och ungas rätt vid tvångsvård.\n",
      "\n",
      "-\n",
      "Input sentence: förslag tilln y lvu.\n",
      "Decoded sentence: förslag till ny lvu.\n",
      "\n",
      "-\n",
      "Input sentence: ett tandvårdsstöd för alla.\n",
      "Decoded sentence: ett tandvårdsstöd för alla.\n",
      "\n",
      "-\n",
      "Input sentence: fler och starkar ptaienter.\n",
      "Decoded sentence: fler och starkare patienter.\n",
      "\n",
      "-\n",
      "Input sentence: organdonation.\n",
      "Decoded sentence: organisationen.\n",
      "\n",
      "-\n",
      "Input sentence: en livsviktig verksamhet.\n",
      "Decoded sentence: en livsviktig verksamhet.\n",
      "\n",
      "-\n",
      "Input sentence: träning ger färdighet.\n",
      "Decoded sentence: träning ger färdighet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
